{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1518905",
   "metadata": {},
   "source": [
    "# AWS re:Invent Smarter Pharmacovigilance with AWS Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc86ea3d",
   "metadata": {},
   "source": [
    "Social media provides an ever-increasing supply of information and users are leaning towards platforms such as Facebook and Twitter to describe their healthcare experiences. This presents another channel and opportunity for customers in Healthcare and Life Sciences to capture, investigate and report Adverse Drug Reactions (ADR) faster and in larger quantities than traditional reporting systems.\n",
    "\n",
    "In this session, we will train and deploy a transformer-based deep learning model, using the [Amazon SageMaker and Hugging Face](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html), to classify ADRs in social media data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796db1b8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664e45c",
   "metadata": {},
   "source": [
    "# Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c70d6",
   "metadata": {},
   "source": [
    "* Introductions - Speakers and Attendees:             [5 mins]\n",
    "* Use case and solution overview:                     [10 mins]\n",
    "    * Identifying Adverse Drug Reaction (ADR)\n",
    "    * Common challenges with datasets and the ML models\n",
    "* Hands-on solution development                       [30 mins]\n",
    "    * Explore the Twitter ADR dataset\n",
    "    * Prepare the dataset for model training\n",
    "    * Fine-tuning a pre-trained BERT model using the Amazon SageMaker HuggingFace estimator\n",
    "    * Deploy the trained model to a Amazon SageMaker endpoint for inference\n",
    "* Discussion, Q&A:                                    [10 mins]\n",
    "* Call-to-action:                                     [5 mins]\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990553fc",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd8eb95",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d85f5",
   "metadata": {},
   "source": [
    "We'll begin by installing the necessary libraries, importing them into the development environment, selecting the appropriate IAM role and the Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf44fab1",
   "metadata": {},
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0299fca",
   "metadata": {},
   "source": [
    "Select the `conda_pytorch_p36` notebook kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec481eb6",
   "metadata": {},
   "source": [
    "Install the required libraries from Hugging Face - `transformers` and `datasets`. We'll also ensure that we have the updated version of `SageMaker Python SDK`\n",
    "\n",
    "Documentation on [Installing Transformers](https://huggingface.co/transformers/installation.html) and [Installing SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip --quiet install \"sagemaker\" \"transformers==4.6.1\" \"datasets==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7963a753",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface\n",
    "import sagemaker\n",
    "\n",
    "# The SageMaker session bucket is used to upload data, models and the corresponding logs.\n",
    "# SageMaker will automatically create this bucket, if it does not exist.\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "#print(f\"SageMaker Role Arn: {role}\")\n",
    "#print(f\"SageMaker - Amazon S3 Bucket: {sess.default_bucket()}\")\n",
    "print(f\"SageMaker Session Region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f544205d",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6691f289",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f275af2",
   "metadata": {},
   "source": [
    "The dataset has been sourced from Twitter and annotated by the Diego Lab, a Biomedical Informatics Lab at Arizona State University (ASU). In the interest of research, the Diego Lab has made a subset of data publicly available.\n",
    "\n",
    "\n",
    "The tweets associated with a set of drugs were collected using the generic and brand names of the drugs, and also their possible phonetic misspellings, since it is common for user posts on Twitter to contain spelling errors. Following the collection of the data, a randomly selected sample was then chosen for annotation, which consisted of 10,822 instances. This dataset is from the year 2016, after which, certain users have either edited or deleted their tweets. The data downloaded for this session contains a total of 5,594 instances. The data was annotated by two domain experts under the guidance of a pharmacology expert. Each tweet is annotated for the presence of ADRs. For more information on the data collection and annotation process, refer to the corresponding research paper - [Portable Automatic Text Classification for Adverse Drug Reaction Detection via Multi-corpus Training](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4355323/).\n",
    "\n",
    "Since, we cannot distribute the tweets as per the Twitter Privacy Policy, we create synthetic tweets that are based on the tweets annotated by the Diego Lab, a Biomedical Informatics Lab at Arizona State University (ASU). To create synthetic tweets, refer to the notebook `create_synthetic_adr_twitter_data.ipynb`.\n",
    "\n",
    "\n",
    "For the rest of the session, we use the synthetic tweets in the file `adr_classify_twitter_synthetic_data.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5264e3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Social media data is often messy and the content could have special characters.\n",
    "# To ensure that we can read the CSV file appropriately through pandas, we explicitly specify the line terminator\n",
    "df = pd.read_csv(\"adr_classify_twitter_synthetic_data.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573be5d5",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fdfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas-profiling[notebook]==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ceec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df, title=\"Twitter ADR Dataset Report\")\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de138349",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048a96e",
   "metadata": {},
   "source": [
    "## Potential data cleaning and transformation operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d7255",
   "metadata": {},
   "source": [
    "In this notebook, we haven't applied any specific data cleaning operations, since the baseline model had a strong performance.\n",
    "\n",
    "But, social media users are creative and the data cleaning operations depend highly on your dataset. Here is a list of data cleaning and transformation operations that we recommend for your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7210e2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef311d",
   "metadata": {},
   "source": [
    "### Convert emojis to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip --quiet install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f926c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji.emojize('AWS re:Invent is :thumbs_up:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b9b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji.demojize('I ‚ù§ Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Is it 630 yet?!? I'm ready for a free dinner and drinks thanks to Effient üíäüíäüíä\"\n",
    "\n",
    "emoji.demojize(s, delimiters=(' ', ' '))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f230c574",
   "metadata": {},
   "source": [
    "df['text'] = df['text'].apply(lambda ele: emoji.demojize(ele, delimiters=(' ', ' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18c367",
   "metadata": {},
   "source": [
    "Similarly, you could use the `emot` library for emoticons. Reference - https://github.com/NeelShah18/emot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad681f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e0b5e",
   "metadata": {},
   "source": [
    "### Detect medical entites and remove PHI using Amazon Comprehend Medical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'9:26 Day 12 Rivaroxaban diary: headache, right shoulder and neck pain, lower back pain, weak knees, limp when walking.Paracetamol taken'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3b9c0",
   "metadata": {},
   "source": [
    "To obtain the visualization as below, navigate to `Amazon Comprehend` from the AWS Management Console; then `Launch Amazon Comprehend Medical` and navigate to the `Real-time analysis` tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"images/comprehend_medical_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5a040",
   "metadata": {},
   "source": [
    "To run the cell below ensure that the IAM role attached to the SageMaker notebook instance has perimissions to invoke the `comprehendmedical:DetectEntitiesV2` API. For more information on attaching the necessary policies and adding trusted entites, refer to the documentation - [Creating a role to delegate permissions to an AWS service](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-service.html)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3892121a",
   "metadata": {},
   "source": [
    "import boto3\n",
    "acmedical_client = boto3.client('comprehendmedical')\n",
    "df['compmed_text'] = df['text'].apply(lambda ele: acmedical_client.detect_entities_v2(Text = ele))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66166058",
   "metadata": {},
   "source": [
    "For a complete list of entitity types within Comprehend Medical, refer to the [documentation](https://docs.aws.amazon.com/comprehend/latest/dg/extracted-med-info-V2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde76d07",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbeae2a",
   "metadata": {},
   "source": [
    "### Additional recommendations\n",
    "\n",
    "\n",
    "1. Remove special characters, URLs, usernames.\n",
    "2. Leverage libraries geared towards cleaning social media text such as - [Ekphrasis](https://github.com/cbaziotis/ekphrasis) for tokenization, word normalization, word segmentation (for splitting hashtags) and spell correction, specifically for social media text.\n",
    "3. If your customer support team responds to candidate adverse event comments on social media, filter the support team's comments out to avoid any bias.\n",
    "4. We may not need the entire text content to make a prediction. Often times in social networks such as Facebook, the comments could be very long. In such scenarios, comments can be cropped to, say 200 tokens, to speed up the data transformations. The maximum length of a comment can be fine-tuned to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c2a14",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c342e0",
   "metadata": {},
   "source": [
    "![](images/arch_aws_consultant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb3fd9",
   "metadata": {},
   "source": [
    "## Discussion: What other transformations would you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4403928",
   "metadata": {},
   "source": [
    "![](images/arch_aws_consultant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce82b499",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a6d008",
   "metadata": {},
   "source": [
    "## Prepare the dataset for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960c863",
   "metadata": {},
   "source": [
    "Create train-validation-test stratified splits of the dataset. Stratification ensures that all classes are equally well represented across the train, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9fcbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = 'label'\n",
    "cols = df.columns[df.columns != target]\n",
    "\n",
    "X = df[cols]\n",
    "y = df[target]\n",
    "\n",
    "# Data split: 5.2%(val) of the 95% (train and test) of the dataset ~ 5%; resulting in 90:5:5 split\n",
    "test_dataset_size = 0.05\n",
    "val_dataset_size = 0.052\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "# Stratified train-val-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_dataset_size, stratify=y, random_state=RANDOM_STATE)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_dataset_size, stratify=y_train, random_state=RANDOM_STATE)\n",
    "\n",
    "print('Dataset: train ', X_train.shape, y_train.shape, y_train.value_counts(dropna=False, normalize=True).to_dict())\n",
    "print('Dataset: val ', X_val.shape, y_val.shape, y_val.value_counts(dropna=False, normalize=True).to_dict())\n",
    "print('Dataset: test ', X_test.shape, y_test.shape, y_test.value_counts(dropna=False, normalize=True).to_dict())\n",
    "\n",
    "# Combine the independent columns with the label\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "df_val = pd.concat([X_val, y_val], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285cf93c",
   "metadata": {},
   "source": [
    "Convert the pandas dataframes into Hugging Face datasets for downstream modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "# REMOVE below since tweet_id and user_id are not present\n",
    "# drop the tweet_id and user_id columns, since we will not use them for modeling\n",
    "# df_train = df_train[['text', 'label']]\n",
    "# df_val = df_val[['text', 'label']]\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_val = Dataset.from_pandas(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e47747",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf34d84",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb066767",
   "metadata": {},
   "source": [
    "Tokenize the `text` field in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_and_model_name = 'roberta-large'\n",
    "\n",
    "# Download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_and_model_name)\n",
    "\n",
    "# Tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "dataset_train_tokenized = dataset_train.map(tokenize, batched=True, batch_size=len(dataset_train))\n",
    "dataset_val_tokenized = dataset_val.map(tokenize, batched=True, batch_size=len(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02960faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_tokenized, dataset_val_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert array/list to PyTorch tensors\n",
    "dataset_train_tokenized = dataset_train_tokenized.rename_column(\"label\", \"labels\")\n",
    "dataset_train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "dataset_val_tokenized = dataset_val_tokenized.rename_column(\"label\", \"labels\")\n",
    "dataset_val_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591a52a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91e3d5",
   "metadata": {},
   "source": [
    "## Upload tokenized dataset to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ec85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "s3_prefix = 'reinvent-builders-session/datasets/adr-twitter'\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "dataset_train_tokenized.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "dataset_val_tokenized.save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbce73",
   "metadata": {},
   "source": [
    "# Model Training: Amazon SageMaker - Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b2fde6",
   "metadata": {},
   "source": [
    "## Overview of transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5faa9d",
   "metadata": {},
   "source": [
    "![](images/bert_transfer_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94417a0",
   "metadata": {},
   "source": [
    "This illustration is from Alammar, J (2021). The Illustrated BERT, ELMo, and co. Retrieved from https://jalammar.github.io/illustrated-bert/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63de61",
   "metadata": {},
   "source": [
    "Other resources for learning more about transfer learning and BERT include - 1) [Recent Advances in Language Model Fine-tuning](https://ruder.io/recent-advances-lm-fine-tuning/) by Sebastian Ruder; 2) [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar; 3) [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805); 4) [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) and 5) [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf2ae1",
   "metadata": {},
   "source": [
    "## Model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a938fc2",
   "metadata": {},
   "source": [
    "Now that we are done with data preparation, we are ready to train our model. We start with a pre-trained model i.e RoBERTa and fine-tune the model for the specific task of identifying ADRs in the dataset. We can leverage the Hugging Face Estimator class within SageMaker to load the pre-trained model, fine-tune it, and managed the end-to-end model training process.\n",
    "\n",
    "\n",
    "Ensure that the `transformers_version`, `pytorch_version` and `py_version` are aligned, as specificed in the [Deep Learning Container (DLC) documentation](https://huggingface.co/docs/sagemaker/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc31bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': 4,\n",
    "                   'train_batch_size': 4,\n",
    "                   'model_name': tokenizer_and_model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "    {'Name': 'loss', 'Regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_loss', 'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_accuracy', 'Regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_f1', 'Regex': \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_precision', 'Regex': \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_recall', 'Regex': \"'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "huggingface_estimator = HuggingFace(\n",
    "                            entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.8xlarge',\n",
    "                            instance_count=1,\n",
    "                            volume_size=100,\n",
    "                            transformers_version='4.6.1',\n",
    "                            pytorch_version='1.7.1',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters=hyperparameters,\n",
    "                            metric_definitions=metric_definitions\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11394505",
   "metadata": {},
   "source": [
    "Start model training by calling the fit method in the estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path}, \n",
    "                          wait=True, \n",
    "                          job_name='sm-pharmacovigilance-training-job-{}'.format(int(time.time())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d576f8f",
   "metadata": {},
   "source": [
    "![](images/sagemaker_training_job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba1c348",
   "metadata": {},
   "source": [
    "## Additional recommendations\n",
    "\n",
    "\n",
    "1. Real-world labels are often noisy i.e. observations that are not adverse events, could be labeled as adverse events; and vice-versa. Noisy labels can mislead the model and degrade its' performance. We have found that smaller, cleaner datasets can outperform large, noisy datasets. You could explore regularization techniques such as label smoothing to prevent the model from learning noisy labels and reduce overfitting. [When Does Label Smoothing Help?](https://arxiv.org/pdf/1906.02629.pdf)\n",
    "\n",
    "2. Real-world datasets for identifing adverse events typically have a class imbalance i.e. only a small percentage of the entire dataset is labeled as adverse events. Methods to handle class imbalace include - applying class weights, upsampling the minority class, downsampling the majority class and augmenting the dataset with techniques such as language translation. In addition, the threshold of probability=0.5 may not be ideal in every scenario; in such cases, you can use the precision-recall curves to identify the optimal threshold.\n",
    "\n",
    "3. [Set a random seed](https://huggingface.co/transformers/internal/trainer_utils.html?highlight=set_seed#transformers.set_seed) to ensure reproducibility during model training. The stochastic nature of many ML algorithms adds additional complexity, because the same dataset along with the same codebase could produce different outputs. This is more pronounced in deep learning algorithms, which make efficient approximations for complex computations. \n",
    "\n",
    "4. To select the instance size during model training, we recommend starting with an ml.p3.2xlarge instance, and increasing the instance size if there isn't sufficient memory.\n",
    "\n",
    "5. Additional components within SageMaker such as - [SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) to organize, track, compare, and evaluate your machine learning experiments, and [SageMaker Distributed Training](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html) can improve the productivity of your model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58b69f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31465505",
   "metadata": {},
   "source": [
    "![](images/arch_aws_consultant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a22d8",
   "metadata": {},
   "source": [
    "## Questions in the Model Training Phase ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967b582",
   "metadata": {},
   "source": [
    "![](images/arch_aws_consultant.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72dab02",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb5591",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a82b7c",
   "metadata": {},
   "source": [
    "To deploy the trained model to an endpoint, we call the `deploy()` method on the HuggingFace estimator object."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1498f298",
   "metadata": {},
   "source": [
    "predictor = huggingface_estimator.deploy(initial_instance_count=1,\n",
    "                                         instance_type=\"ml.g4dn.xlarge\", \n",
    "                                         endpoint_name=\"reinvent-2021-pharmacovigilance-builders-session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67ee53",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf31b6",
   "metadata": {},
   "source": [
    "Since RoBERTa is a large pre-trained model and we are running it through multiple epochs, the training job takes an hour to complete. In the interest of time, let's load the model - with the same configuration and the same dataset, which was trained prior to the session.\n",
    "\n",
    "Ensure that the `transformers_version`, `pytorch_version` and `py_version` are aligned, as specificed in the [Deep Learning Container (DLC) documentation](https://huggingface.co/docs/sagemaker/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3452bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "latest_sm_training_job_name = sm_client.list_training_jobs()['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "\n",
    "S3_PATH_TRAINED_MODEL_FILE = 's3://' + sagemaker_session_bucket + '/' + latest_sm_training_job_name + '/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be82a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=S3_PATH_TRAINED_MODEL_FILE,\n",
    "    role=role,\n",
    "    transformers_version='4.6.1',\n",
    "    pytorch_version='1.7.1',\n",
    "    py_version='py36',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_model.deploy(initial_instance_count=1,\n",
    "                                     instance_type='ml.g4dn.xlarge', \n",
    "                                     endpoint_name='reinvent-2021-pharmacovigilance-builders-session')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14e0bc",
   "metadata": {},
   "source": [
    "![](images/sagemaker_endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c13e02",
   "metadata": {},
   "source": [
    "Other scenarios to deploy a model to a SageMaker endpoint include - 1) from a model stored in the [Hugging Face Hub](https://huggingface.co/models) and 2) by using a custom inference container. For more information on these methods refer to [Announcing managed inference for Hugging Face models in Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/announcing-managed-inference-for-hugging-face-models-in-amazon-sagemaker/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2487dda7",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec27059",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e2863",
   "metadata": {},
   "source": [
    "Once the model is deployed, we can send observations from the unseen test dataset - `df_test` to the endpoint, to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f0ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8def98e",
   "metadata": {},
   "source": [
    "Let's select a few sentences from the test dataset and send it to the endpoint for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = 'So my body was goin threw withdrawl? Lol didnt kno Paxil was that strong lol'\n",
    "\n",
    "sent_2 = 'Sometimes my Humira pens make me feel like 90 years old everywhere because of the joint pain.'\n",
    "\n",
    "sent_3 = 'In other news, I have my first strange side effect of quetiapine. Numbness and tingling in my fingertips.'\n",
    "\n",
    "sent_4 = 'Tysabri should be a treatment for spms when studies prove effective and at least 3 desperate stem cell trials are underway. Incredible work!'\n",
    "\n",
    "tweets= {\"inputs\": [sent_1, sent_2, sent_3, sent_4]}\n",
    "\n",
    "predictor.predict(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69eec9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ddf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['pred'] = df_test['text'].apply(lambda ele: predictor.predict({\"inputs\": ele})[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9fe64b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636aa284",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f6734",
   "metadata": {},
   "source": [
    "When we are done with the endpoint, we can delete it to save cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b19bf0",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f4f3c",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1c08a",
   "metadata": {},
   "source": [
    "1. [Use Hugging Face with Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html)\n",
    "2. [Hugging Face sample notebooks](https://github.com/huggingface/notebooks/tree/master/sagemaker)\n",
    "3. [AWS Blog - AWS and Hugging Face collaborate to simplify and accelerate adoption of Natural Language Processing models](https://aws.amazon.com/blogs/machine-learning/aws-and-hugging-face-collaborate-to-simplify-and-accelerate-adoption-of-natural-language-processing-models/)\n",
    "4. [AWS Blog - Announcing managed inference for Hugging Face models in Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/announcing-managed-inference-for-hugging-face-models-in-amazon-sagemaker/)\n",
    "5. [The Partnership: Amazon SageMaker and Hugging Face](https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face)\n",
    "6. Sarker A, Gonzalez G. Portable automatic text classification for adverse drug reaction detection via multi-corpus training. J Biomed Inform. 2015;53:196-207. doi:10.1016/j.jbi.2014.11.002\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e7c91",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
